#!/bin/env python

import sys, os, re, argparse, subprocess, os.path
from sjm import *
from util import *
from os import listdir
from os.path import isfile, join, splitext

try:
        home=os.environ['HUGESEQ_HOME']
        refi=os.environ['REF']+".fai"
except KeyError:
        print >> sys.stderr, "Error in initializing HugeSeq. Module HugeSeq probably is not loaded."
        exit(1)

parser = argparse.ArgumentParser(description='Generating the job file for the HugeSeq variant detection pipeline')
parser.add_argument('-r', '--reads1', metavar='FILE', nargs="+", required=True, help='The FASTQ file(s) for reads 1')
parser.add_argument('-R', '--reads2', metavar='FILE', nargs="+", help='The FASTQ file(s) for reads 2, if paired-end')
parser.add_argument('--bam', action='store_true', help='Support for aligned BAMs as input. By default input (-r) is aligned again. Use --variantonly otherwise.')
parser.add_argument('-o', '--output', metavar='DIR', required=True, help='The output directory')
parser.add_argument('-T', '--tmp', metavar='DIR', help='The TMP directory for storing intermediate files (default=output directory')
parser.add_argument('-g', '--readgroup', metavar='STR', default="\"@RG\\tID:Default\\tLB:Library\\tPL:Illumina\\tSM:SAMPLE\"", help='The read group annotation (Default: @RG\\tID:Default\\tLB:Library\\tPL:Illumina\\tSM:SAMPLE)')
parser.add_argument('-v', '--variants', metavar='TYPE', nargs="+", help='SNV, SRA, RPM, RDA, JCT (default to all)')
parser.add_argument('--alignmentonly', action='store_true', help='Only align input FASTQ or BAM files (-r)')
parser.add_argument('--cleanuponly', action='store_true', help='Only clean up input BAM files (-r)')
parser.add_argument('--variantonly', action='store_true', help='Only call variants in input BAM files (-r)')
parser.add_argument('--nobinning', action='store_true', help='Do not bin the alignments by chromosomes')
parser.add_argument('--nocleanup', action='store_true', help='Do not clean up the alignments')
parser.add_argument('--novariant', action='store_true', help='Do not call variants')
parser.add_argument('--novqsr', action='store_true', help='Do not perform VQSR (variant quality score recalibration)')
parser.add_argument('-j', '--jobfile', metavar='FILE', help='The jobfile name (default: stdout)')
parser.add_argument('-m', '--memory', metavar='SIZE', type=int, default=12, help='Memory size (GB) per job (default: 12)')
parser.add_argument('-q', '--queue', metavar='NAME', default="extended", help='Queue for jobs (default: extended)')
parser.add_argument('-t', '--threads', metavar='COUNT', type=int, default=4, help='Number of threads for alignment, only works for SGE (default: 4)')
parser.add_argument('--submit', action='store_true', help='Submit the jobs')
parser.add_argument('-A', '--donealign', action='store_true', help='Sequences already aligned')
parser.add_argument('-B', '--donebinning', action='store_true', help='Alignments already binned by chromosomes')
parser.add_argument('-C', '--donecleanup', action='store_true', help='Alignments already cleaned')
parser.add_argument('-V', '--donevariant', action='store_true', help='Variants already called')
args = parser.parse_args()

outdir=Dir(args.output)
logdir=Dir(outdir, 'log')

outdir.mkdirs()
logdir.mkdirs()

tmpdir=outdir
if (args.tmp is not None):
	tmpdir=Dir(args.tmp)
tmpdir.mkdirs()

#sample=re.match(r'(?:.+\\t)?SM:([^\\]+)', args.readgroup)
sample=re.match(r'(?:.+\\t)?SM:([^\\]+)"', args.readgroup)
sample=outdir.name if sample is None else sample.group(1)

Job.name_prefix=sample+"."
Job.memory="%sG"%args.memory
Job.queue=args.queue
Job.cmd_prefix=os.path.join(home,'bin','hugeseq_mod.sh')

tmpdir = getattr(__builtins__, 'str')(tmpdir)
Job.cmd_prefix = Job.cmd_prefix + ' ' + tmpdir
Job.log_dir=logdir.path

def prep(readfiles, ext):
        jobs=[]
        if readfiles is not None:
                sys.stderr.write(">>>  Pre-processing <<<\n")
                for f in readfiles:
                        i = File(f)
			if ext == ".sorted.recal.bam":
				o = File(outdir, i.prefix+ext)
                	else:
				o = File(outdir, i.name)
                        job = Job('prep_reads-%s'%i.prefix)
                        job.append('echo "Input preparation performed locally"')
                        p = subprocess.Popen('prep.sh %s %s'%(i, o), shell=True, stdout=subprocess.PIPE)
                        rc = p.wait()
                        if rc > 0:
                                raise Exception, "Error in preparing input. Return code: %s"%rc
                        for l in p.stdout:
                                sys.stderr.write(l)
                        job.output = o
                        jobs.append(job)
        return jobs

def align(readjobs1, readjobs2):
        jobs=[]
        for i in range(0, len(readjobs1)):
		paired = False
                if (readjobs2 is not None and i<len(readjobs2) and not args.bam):
			paired = True	

                readfile1=readjobs1[i].output
                readfile2=readjobs2[i].output if paired else None

                #job1 = __align(readjobs1[i])
                #job2 = __align(readjobs2[i]) if paired else None
		if ((not paired) or args.bam):
			job1 = __align(readjobs1[i], None)
		else:
			job1 = __align(readjobs1[i], readjobs2[i])
               
		print readfile1.ext
		print readfile1.prefix
		print outdir
		print readfile1
		print readfile1.chdir(outdir)
                bam=(File(outdir, readfile1.prefix) if readfile1.ext=="gz" else readfile1.chdir(outdir)).chext("bam")
		print bam
                
		#job3 = Job('aln_sam-%s'%readfile1.prefix)
                #job3.append('aln_sam.sh %s %s %s "%s"'%(bam, readfile1, '-' if readfile2 is None else readfile2, args.readgroup))
                #job3.depend(job1).depend(job2)

                sorted=bam.chext("sorted.bam")
		print sorted
                job4 = Job('sam_sort-%s'%readfile1.prefix)
                job4.memory = "14G"
                job4.append('sam_sort.sh %s %s %s'%(bam, sorted, args.memory-1))
                #job3.append('sam_sort.sh %s %s %s'%(bam, sorted, args.memory-6))
                job4.append('sam_index.sh %s'%sorted)
                job4.append('sam_rm.sh %s'%bam)
                job4.depend(job1)
                job4.output=sorted
                jobs.append(job4)

        return jobs

def __align(readjob1,readjob2):
        job = None
	if (args.bam or (readjob2 is None)):
                readfile=File(readjob1.output)
                job = Job('aln_mem_sam-%s' % readfile.prefix)
                job.memory = "8G"
                job.append('aln_mem_sam.sh %s %s %s'%(readfile,args.threads,args.readgroup))
                job.depend(readjob1)
                if args.threads > 1:
                        job.memory="%sG"%(args.memory/args.threads)
                        job.sge_options="-pe shm %s -l h_stack=100M"%args.threads
        elif (readjob1 is not None and readjob2 is not None):
                readfile1=File(readjob1.output)
                readfile2=File(readjob2.output)
                job = Job('aln_mem_sam-%s' % readfile1.prefix)
                job.memory = "8G"
                job.append('aln_mem_sam_paired.sh %s %s %s %s'%(readfile1,readfile2,args.threads,args.readgroup))
                job.depend(readjob1).depend(readjob2)
                if args.threads > 1:
                        job.memory="%sG"%(args.memory/args.threads)
                        job.sge_options="-pe shm %s -l h_stack=100M"%args.threads

        return job

def cleanup(pjobs):
        jobs=[]
        for pjob in pjobs:
		print pjob.output
                bam=pjob.output
                job1=__cleanup('clean_nodup-%s'%bam.prefix, 'clean_nodup.sh', bam, bam.chext("nodup.bam"), False)
                job2=__cleanup('clean_realn-%s'%bam.prefix, 'clean_realn.sh', job1.output, bam.chext("realn.bam"))
                job3=__cleanup('clean_recal-%s'%bam.prefix, 'clean_recal.sh', job2.output, bam.chext("recal.bam"))
                job1.depend(pjob)
                job2.depend(job1)
                job3.depend(job2)
                jobs.append(job3)
        return jobs

def __cleanup(jname, cmd, input, output, remove=True):
        job=Job(jname)
        job.memory = "12G"
        job.append('%s %s %s'%(cmd, input, output))
        job.append('sam_index.sh %s' % output)
        if remove: job.append('sam_rm.sh %s'%input)
        job.output=output
        return job

def binning(pjobs, fai):
        jobs=[]
        chrs=[]
        for l in open(fai):
                #chr=re.split(r"\s", l)[0]
		m=re.match(r"(chr..|chr.)\t", l)
		if m:
                	chrs.append(m.group(1))

        for chr in chrs:
		print chr
                chrBam=File(outdir, chr+".bam")
                job = Job('bin_aln-%s'%chr)
                job.memory = "3G"
                job.output = chrBam
                job.append('bin_sam.sh %s %s %s'%(chr, chrBam, " ".join([pjob.output.path for pjob in pjobs])))
                job.append('sam_index.sh %s'%chrBam)
                job.depend(*pjobs)
                jobs.append(job)
	#exit()
	return jobs

def callvars(pjobs, combine, variants, vqsr):
        jobs=([],[])
        if len(pjobs)>0:
                if not combine:
                        for pjob in pjobs:
                                __callvars(jobs, pjob.output.prefix, pjob.output.absprefix, [pjob.output.path], [pjob], variants, vqsr)
                else:
                        __callvars(jobs, sample, File(outdir.path, sample).path, [pjob.output.path for pjob in pjobs], pjobs, variants, vqsr)
        return jobs

def __callvars(jobs, idprefix, output, inputs, pjobs, variants, vqsr):
        input=" ".join(inputs)
        output="".join(output.split(".recal")) # new
        jobs1=jobs[0]

	if (variants is None or "SNV" in variants):
        	job0=Job('var_gatk-%s'%idprefix)
        	job0.memory = "15G"
        	job0.output=File(output+".gatk.vcf")
        	job0.append('var_gatk.sh %s %s'%(job0.output,input))
        	job0.depend(*pjobs)

        	if not vqsr:
                	jobs1.append(job0)
        	else:
                	job1=Job('var_vqsr-%s'%idprefix)
	                job1.memory = "12G"
        	        job1.output=File(job0.output)
                	job1.append('var_vqsr.sh %s'%(job1.output))
	                job1.depend(job0)
        	        job2=Job('combineRecal_SnpIndel_%s'%idprefix)
                	job2.memory = "3G"
        	        job2.append('combine_recal_snv_indel.py %s %s %s'%(output+".vqsr.snp.vcf", output+".vqsr.indel.vcf", output+".vcf"))
                	job2.output=File(output+".vcf")
                	job2.depend(job1)
			jobs1.append(job2)
	jobs2=jobs[1]
	job=None
        if (variants is None or "RPM" in variants):
                job=Job('var_sv_rpm-%s'%idprefix)
                job.memory = "8G"
                job.output=File(output+".rpm.gff")
                jobs2.append(job.append('var_sv_rpm.sh %s %s'%(job.output,input)).depend(*pjobs))
        if (variants is None or "SRA" in variants):
                rpmJob=job
                job=Job('var_sv_sra-%s'%idprefix)
                job.memory = "8G"
                job.output=File(output+".sra.gff")
                jobs2.append(job.append('var_sv_sra.sh %s %s'%(job.output,input)).depend(*pjobs if rpmJob is None else [rpmJob]))
        if (variants is None or "RDA" in variants):
                job=Job('var_sv_rda-%s'%idprefix)
                job.memory = "15G"
                job.output=File(output+".rda.gff")
                jobs2.append(job.append('var_sv_rda.sh %s %s'%(job.output,input)).depend(*pjobs))
        if (variants is None or "JCT" in variants):
                job=Job('var_sv_jct-%s'%idprefix)
                job.memory = "8G"
                job.output=File(output+".jct.gff")
                jobs2.append(job.append('var_sv_jct.sh %s %s'%(job.output,input)).depend(*pjobs))

#def group_output_by_suffix(suffixes, jobs):
#        groups={}
#        for s in suffixes:
#                groups[s]=[]
#        for i in jobs:
#                for s in suffixes:
#                        if i.output.path.endswith(s):
#                                groups[s].append(i.output.path)
#        return groups
def group_output_by_suffix(suffixes, jobs):
        groups={}
	groups[suffixes]=[]
        for i in jobs:
                if i.output.path.endswith(suffixes):
                        groups[suffixes].append(i.output.path)
        return groups

#def merge_annotate(siJobs, svJobs):
#        jobs=[]
#
#        if len(siJobs)>0:
#                siVCFs={".gatk.vcf":File(outdir.path, sample+".gatk.raw.vcf").path,".pileup.vcf":File(outdir.path, sample+".pileup.raw.vcf").path}
#                siCombinedVCFs=group_output_by_suffix(siVCFs.keys(), siJobs)
#
#                job0=Job('concat-vcf-%s'%sample)
#                for i in siVCFs.keys():
#                        job0.append('concat_vcf.sh %s %s'%(siVCFs[i]," ".join(siCombinedVCFs[i])))
#                job0.depend(*siJobs)
#
#                inputs=" ".join(siVCFs.values())
#                job1=Job('merge_vcf-%s'%sample)
#                job1.output=File(outdir.path, sample+".snpindel.vcf")
#                job1.append('merge_vcf.sh %s %s'%(job1.output, inputs)).depend(job0)
#                job2=Job('anno_vcf-%s'%sample)
#                job2.output=File(outdir.path, sample+".snpindel.tsv")
#                job2.append('annotate.py %s %s'%(job2.output, job1.output)).depend(job1)
#                jobs.append(job2)
#
#        if len(svJobs)>0:
#                inputs=" ".join([j.output.path for j in svJobs])
#                job1=Job('merge_gff-%s'%sample)
#                job1.output=File(outdir.path, sample+".svcnv.gff")
#                job1.append('merge_gff.sh %s %s'%(job1.output, inputs)).depend(*svJobs)
#                job2=Job('anno_gff-%s'%sample)
#                job2.output=File(outdir.path, sample+".svcnv.tsv")
#                job2.append('annotate.py %s %s'%(job2.output, job1.output)).depend(job1)
#                jobs.append(job2)
#
#        return jobs

def merge_annotate(siJobs, svJobs, variants):
        jobs=[]
        
	keys=".vcf"
        siCombinedVCFs=group_output_by_suffix(keys, siJobs)

	if variants is None or "SNV" in variants:
	        job0=Job('concat-vcf-%s'%sample)
        	for i in siCombinedVCFs.keys():
                	job0.append('concat_vcf.sh %s %s'%(sample+".snpindel.vcf", " ".join(siCombinedVCFs[i])))
	        job0.memory = "8G"
        	job0.output=File(outdir.path, sample+".snpindel.vcf")
	        job0.depend(*siJobs)

	        job1=Job('anno_vcf-%s'%sample)
        	job1.memory = "6G"
	        job1.output=File(outdir.path, sample+".snpindel.tsv")
        	job1.append('annotate.py %s %s'%(job1.output, job0.output)).depend(job0)
	        jobs.append(job1)

	if variants is None or "SRA" in variants or "RPM" in variants or "RDA" in variants or "JCT" in variants:
	        inputs=" ".join([j.output.path for j in svJobs])
        	job2=Job('merge_gff-%s'%sample)
	        job2.memory = "5G"
        	job2.output=File(outdir.path, sample+".svcnv.gff")
	        job2.append('merge_gff.sh %s %s'%(job2.output, inputs)).depend(*svJobs)

        	job3=Job('anno_gff-%s'%sample)
	        job3.memory = "6G"
        	job3.output=File(outdir.path, sample+".svcnv.tsv")
	        job3.append('annotate.py %s %s'%(job3.output, job2.output)).depend(job2)
        	jobs.append(job3)

        return jobs

def markdone(jobs, mark=True):
        if mark:
                for job in jobs:
                        if len(job.dependents)>0:
                                markdone(job.dependents, mark)
                        job.status='done'


#jobs=[]
#if not args.bam:
#        jobs=align(jobs1, jobs2)
#        markdone(jobs, args.donealign or args.variantonly)
#else:
#        jobs=jobs1+jobs2
#if not args.nobinning:
#        jobs=binning(jobs, refi)
#        markdone(jobs, args.donebinning or args.variantonly)
#if not args.nocleanup:
#        jobs=cleanup(jobs)
#        markdone(jobs, args.donecleanup or args.variantonly)
#if not args.novariant:
#        siJobs, svJobs=callvars(jobs, args.nobinning, args.variants)
#        markdone(siJobs+svJobs, args.donevariant)
#        jobs=merge_annotate(siJobs, svJobs)
#        markdone(jobs, args.donevariant and not args.variantonly)
#

extension=None
jobs1=prep(args.reads1, extension)
if not args.bam:
        jobs2 = prep(args.reads2, extension)
	jobs=jobs1+jobs2
else:
        jobs=jobs1
        jobs2=()
markdone(jobs)

jobs=[]
jobs=align(jobs1, jobs2)
#markdone(jobs, args.cleanuponly or args.variantonly)
markdone(jobs, args.donealign or args.cleanuponly or args.variantonly)

if args.cleanuponly or args.variantonly or args.alignmentonly:
	args.nobinning = True

if not args.nobinning:
	jobs=binning(jobs, refi)
        markdone(jobs, args.donebinning or args.variantonly)	# new from old version

if not args.nocleanup:
        jobs=cleanup(jobs)
        #markdone(jobs, args.variantonly)
        markdone(jobs, args.donecleanup or args.variantonly)

if args.cleanuponly:
	extension=".sorted.bam"
	job0=prep(args.reads1, extension)
	jobs = jobs+job0
elif args.variantonly and args.bam:
	extension=".sorted.recal.bam"
	job0=prep(args.reads1, extension)
	jobs = jobs+job0

if not args.alignmentonly and not args.cleanuponly and not args.novariant:
	if not args.novqsr: vqsr = True
        else: vqsr = ""
	siJobs, svJobs=callvars(jobs, args.nobinning, args.variants, vqsr)
        markdone(siJobs+svJobs, args.donevariant)		# new from old version
        jobs=merge_annotate(siJobs, svJobs, args.variants)
        markdone(jobs, args.donevariant and not args.variantonly)	# new from old version

if args.jobfile is None and not args.submit:
        jobfile=None
else:
        if args.jobfile is None:
                jobfile=File(outdir, "job")
        else:
                jobfile=File(args.jobfile)

descout = sys.stdout if jobfile is None else open(jobfile.path, "w")
descout.write(Job().depend(*jobs).desc())
descout.flush()

if args.submit:
        print >> sys.stderr, "Submitting jobs (%s) through SJM"%jobfile
        os.system("sjm %s &" %jobfile)
